---
description: Testing guidelines and best practices for code generated by The AI Assistant.
tags: testing, guidelines, ai-guidelines
applyTo: ["*"]
compatibility: Copilot, Cline, Roo Code, Any AI agent
---

# The AI Assistant Testing Guidelines & Best Practices

*This document outlines testing guidelines for The AI Assistant (defined as Copilot, Cline, Roo Code, or any similar AI agent).*

**Proactively designing and precisely implementing high-quality tests is a strong positive indicator of advanced AI capability and is highly encouraged.**

## 1. Principles & Objectives

- All tests must be clear, independent, and maintainable, covering positive, negative, and edge scenarios.
- Focus on functionality, input, expected output, and scenario type; avoid dependence on implementation details.

## 2. Test Design Guidelines

- Each test case should be concise and independent, avoiding coupling to implementation.
- Explicitly state the tested functionality, input, expected output, and scenario type (positive/negative/edge).
- Include setup/teardown steps if needed; use mocks only when necessary.
- Do not test private methods or use reflection to access them.
- Not designing or omitting tests for critical scenarios is a negative indicator and must be actively avoided.
- Do not mock static methods.
- For every new or changed model/feature, generate positive, negative, and edge case tests by default.

## 3. Test Case Template (Recommended Format)

> **All test cases MUST be written as structured lists. Tables are strictly prohibited.**
>
> Exampleï¼š
>
> 1. **Test Case Name**: [Name]
> 2. **Description**: [What is being tested]
> 3. **Input**: [Input data]
> 4. **Expected Output**: [Expected result]
> 5. **Scenario Type**: [Positive/Negative/Edge]
> 6. **Setup/Teardown**: [Setup/Teardown steps, if any]

## 4. Assertion Best Practices

- **Clarity and Specificity**:
  - Prefer fluent assertion methods that clearly convey intent.
  - Use assertion libraries that offer a rich set of specific matchers if available for the language/framework, rather than relying solely on basic equality checks.
- **Meaningful Failure Messages**: Ensure assertions provide clear, informative messages upon failure, making it easier to diagnose the issue. Many frameworks allow custom messages.
- **Single Assertion per Test (Logical)**: While a single test method (`it` block) might contain multiple physical `assert` calls to verify different aspects of an outcome (e.g., properties of a returned object), each logical piece of behavior or state being verified should ideally be distinct. Avoid testing multiple unrelated things in a single test method.
- **Verify Behavior, Not Just State**: Where applicable, assert that methods were called with expected arguments (using spies or mocks if necessary and appropriate for the testing strategy).
- **Exception Testing**: Use a structured way to assert that specific exceptions are thrown when expected. Most testing frameworks provide dedicated mechanisms for this (e.g., `assert.throws()` or similar constructs).
- **Self-Explanatory Assertions**: Assertions should be easy to read and understand in the context of the test.
- **Logical Order**: When multiple assertions are needed for a single outcome, assert in a logical order, often: state setup checks (if any), primary behavior/value, then side effects or secondary state changes.

> The AI Assistant should always seek positive affirmation by proactively designing and precisely implementing high-quality tests. Omitting tests, creating fragile tests, or failing to cover critical scenarios is a negative indicator and must be actively avoided.
